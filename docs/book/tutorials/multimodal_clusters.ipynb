{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Clusters: DDC Spatial Coverage\n",
        "\n",
        "This notebook demonstrates DDC's advantage in **clustered data** with multiple well-separated modes. ",
        "We'll show how DDC preserves cluster structure better than Random sampling.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- DDC preserves cluster structure better than Random\n",
        "- Spatial coverage matters for clustered data\n",
        "- How to interpret coverage metrics\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "We'll use a **Gaussian mixture with 6 clusters** of varying sizes. ",
        "This demonstrates DDC's ability to ensure all clusters are represented, even small ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dd-coresets (uncomment if needed)\n",
        "# !pip install dd-coresets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "try:\n",
        "    import umap\n",
        "    HAS_UMAP = True\n",
        "except ImportError:\n",
        "    HAS_UMAP = False\n",
        "    print(\"UMAP not available, using PCA for visualization\")\n",
        "\n",
        "from dd_coresets import fit_ddc_coreset, fit_random_coreset\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate Dataset\n",
        "\n",
        "We'll create a Gaussian mixture with **6 clusters** of varying sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate Gaussian mixture with imbalanced clusters\n",
        "n_samples = 15000\n",
        "n_features = 10\n",
        "n_clusters = 6\n",
        "\n",
        "# Cluster sizes: [5000, 3000, 2000, 2000, 1500, 1500]\n",
        "cluster_sizes = [5000, 3000, 2000, 2000, 1500, 1500]\n",
        "\n",
        "X_list = []\n",
        "labels_list = []\n",
        "\n",
        "for i, size in enumerate(cluster_sizes):\n",
        "    X_cluster, _ = make_blobs(\n",
        "        n_samples=size,\n",
        "        n_features=n_features,\n",
        "        centers=1,\n",
        "        cluster_std=1.2,\n",
        "        center_box=(-15, 15),\n",
        "        random_state=RANDOM_STATE + i\n",
        "    )\n",
        "    # Offset each cluster\n",
        "    offset = np.array([i * 8, 0] + [0] * (n_features - 2))\n",
        "    X_cluster += offset\n",
        "    X_list.append(X_cluster)\n",
        "    labels_list.append(np.full(size, i))\n",
        "\n",
        "X = np.vstack(X_list)\n",
        "cluster_labels = np.concatenate(labels_list)\n",
        "\n",
        "# Shuffle\n",
        "indices = np.random.permutation(len(X))\n",
        "X = X[indices]\n",
        "cluster_labels = cluster_labels[indices]\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of clusters: {n_clusters}\")\n",
        "print(f\"\\nCluster sizes:\")\n",
        "for i in range(n_clusters):\n",
        "    size = np.sum(cluster_labels == i)\n",
        "    print(f\"  Cluster {i}: {size:,} samples ({size/len(X):.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fit Coresets\n",
        "\n",
        "We'll create coresets using DDC and Random sampling with `k=300` representatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 300\n",
        "\n",
        "print(\"Fitting coresets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# DDC\n",
        "S_ddc, w_ddc, info_ddc = fit_ddc_coreset(\n",
        "    X, k=k, mode='euclidean', preset='balanced', random_state=RANDOM_STATE\n",
        ")\n",
        "print(f\"\u2713 DDC: {len(S_ddc)} representatives\")\n",
        "\n",
        "# Random\n",
        "S_random, w_random, info_random = fit_random_coreset(\n",
        "    X, k=k, random_state=RANDOM_STATE + 1\n",
        ")\n",
        "print(f\"\u2713 Random: {len(S_random)} representatives\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Spatial Coverage\n",
        "\n",
        "We'll compute how many coreset points are in each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find cluster labels for coreset points\n",
        "nn = NearestNeighbors(n_neighbors=1)\n",
        "nn.fit(X)\n",
        "\n",
        "_, idx_ddc = nn.kneighbors(S_ddc)\n",
        "_, idx_random = nn.kneighbors(S_random)\n",
        "\n",
        "labels_ddc = cluster_labels[idx_ddc.flatten()]\n",
        "labels_random = cluster_labels[idx_random.flatten()]\n",
        "\n",
        "# Count points per cluster\n",
        "coverage_ddc = {}\n",
        "coverage_random = {}\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    n_cluster = np.sum(cluster_labels == cluster_id)\n",
        "    n_ddc = np.sum(labels_ddc == cluster_id)\n",
        "    n_random = np.sum(labels_random == cluster_id)\n",
        "    \n",
        "    coverage_ddc[cluster_id] = n_ddc / k\n",
        "    coverage_random[cluster_id] = n_random / k\n",
        "    \n",
        "    print(f\"Cluster {cluster_id} ({n_cluster:,} samples, {n_cluster/len(X):.1%}):\")\n",
        "    print(f\"  DDC:    {n_ddc:3d} points ({coverage_ddc[cluster_id]:.1%})\")\n",
        "    print(f\"  Random: {n_random:3d} points ({coverage_random[cluster_id]:.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "Let's visualize the coresets and their spatial coverage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project to 2D\n",
        "if HAS_UMAP:\n",
        "    reducer = umap.UMAP(n_components=2, random_state=RANDOM_STATE)\n",
        "else:\n",
        "    reducer = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "\n",
        "X_2d = reducer.fit_transform(X)\n",
        "S_ddc_2d = reducer.transform(S_ddc)\n",
        "S_random_2d = reducer.transform(S_random)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for ax, S_2d, labels, title in zip(\n",
        "    axes, [S_ddc_2d, S_random_2d],\n",
        "    [labels_ddc, labels_random],\n",
        "    ['DDC Coreset', 'Random Coreset']\n",
        "):\n",
        "    # Background: full data\n",
        "    ax.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, \n",
        "              cmap='viridis', alpha=0.1, s=5)\n",
        "    # Coreset points\n",
        "    scatter = ax.scatter(S_2d[:, 0], S_2d[:, 1], c=labels,\n",
        "                        cmap='viridis', s=50, alpha=0.8, \n",
        "                        edgecolors='black', linewidths=0.5)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Component 1')\n",
        "    ax.set_ylabel('Component 2')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coverage comparison bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(n_clusters)\n",
        "width = 0.35\n",
        "\n",
        "ddc_vals = [coverage_ddc[i] for i in range(n_clusters)]\n",
        "random_vals = [coverage_random[i] for i in range(n_clusters)]\n",
        "\n",
        "ax.bar(x - width/2, ddc_vals, width, label='DDC', alpha=0.7, color='blue')\n",
        "ax.bar(x + width/2, random_vals, width, label='Random', alpha=0.7, color='orange')\n",
        "\n",
        "ax.set_xlabel('Cluster ID')\n",
        "ax.set_ylabel('Coreset Coverage (%)')\n",
        "ax.set_title('Spatial Coverage per Cluster', fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([f'C{i}' for i in range(n_clusters)])\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Takeaways\n",
        "\n",
        "### When DDC is Better\n",
        "\n",
        "- **Clustered data**: DDC ensures all clusters are represented\n",
        "- **Imbalanced clusters**: DDC covers small clusters better than Random\n",
        "- **Spatial coverage**: DDC guarantees coverage of all modes\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try `adaptive_distances.ipynb` for advanced features\n",
        "- Try `label_aware_classification.ipynb` for supervised problems\n",
        "- See `docs/DDC_ADVANTAGE_CASES.md` for comprehensive analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}