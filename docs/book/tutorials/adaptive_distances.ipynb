{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Distances & Presets: Advanced DDC Features\n",
    "\n",
    "This notebook demonstrates **adaptive distances** and **pipeline presets** in `dd-coresets` v0.2.0. ",
    "We'll show when to use adaptive Mahalanobis distances and how presets simplify configuration.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- When to use adaptive distances (elliptical clusters, d \u2265 20)\n",
    "- Understanding presets: `fast`, `balanced`, `robust`\n",
    "- Auto mode for dimensionality handling\n",
    "- How adaptive distances improve density estimation\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We'll use **elliptical clusters** where adaptive distances show clear advantage over Euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dd-coresets (uncomment if needed)\n",
    "# !pip install dd-coresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"UMAP not available, using PCA for visualization\")\n",
    "\n",
    "from dd_coresets import fit_ddc_coreset\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Elliptical Clusters\n",
    "\n",
    "We'll create clusters with **elliptical shapes** where adaptive distances excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate isotropic clusters first\n",
    "n_samples = 8000\n",
    "n_features = 15  # Medium dimensionality\n",
    "n_clusters = 3\n",
    "\n",
    "X, cluster_labels = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    centers=n_clusters,\n",
    "    cluster_std=1.0,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Make clusters elliptical by scaling and rotating\n",
    "for cluster_id in range(n_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    X_cluster = X[mask]\n",
    "    \n",
    "    # Create transformation matrix (elliptical)\n",
    "    # Scale first 5 dimensions more\n",
    "    scale = np.ones(n_features)\n",
    "    scale[:5] = 3.0  # Elongate first 5 dims\n",
    "    scale[5:] = 0.5  # Compress remaining dims\n",
    "    \n",
    "    # Apply scaling\n",
    "    X_cluster_scaled = X_cluster * scale\n",
    "    \n",
    "    # Center and replace\n",
    "    X[mask] = X_cluster_scaled - X_cluster_scaled.mean(axis=0) + X_cluster.mean(axis=0)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Features: {n_features} (medium dimensionality, good for adaptive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Euclidean vs Adaptive vs Auto\n",
    "\n",
    "We'll fit DDC with different modes and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200\n",
    "\n",
    "print(\"Fitting coresets with different modes...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Euclidean (default)\n",
    "S_euclidean, w_euclidean, info_euclidean = fit_ddc_coreset(\n",
    "    X, k=k, mode='euclidean', preset='balanced', random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"\u2713 Euclidean: {len(S_euclidean)} representatives\")\n",
    "print(f\"  Pipeline: {info_euclidean['pipeline']}\")\n",
    "\n",
    "# Adaptive\n",
    "S_adaptive, w_adaptive, info_adaptive = fit_ddc_coreset(\n",
    "    X, k=k, mode='adaptive', preset='balanced', random_state=RANDOM_STATE + 1\n",
    ")\n",
    "print(f\"\u2713 Adaptive: {len(S_adaptive)} representatives\")\n",
    "print(f\"  Pipeline: {info_adaptive['pipeline']}\")\n",
    "\n",
    "# Auto (should choose adaptive for d=15)\n",
    "S_auto, w_auto, info_auto = fit_ddc_coreset(\n",
    "    X, k=k, mode='auto', preset='balanced', random_state=RANDOM_STATE + 2\n",
    ")\n",
    "print(f\"\u2713 Auto: {len(S_auto)} representatives\")\n",
    "print(f\"  Pipeline: {info_auto['pipeline']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Presets\n",
    "\n",
    "Let's compare different presets: `fast`, `balanced`, `robust`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presets = ['fast', 'balanced', 'robust']\n",
    "results_presets = {}\n",
    "\n",
    "for preset in presets:\n",
    "    S, w, info = fit_ddc_coreset(\n",
    "        X, k=k, mode='adaptive', preset=preset, random_state=RANDOM_STATE + 10\n",
    "    )\n",
    "    results_presets[preset] = {\n",
    "        'S': S, 'w': w, 'info': info,\n",
    "        'm_neighbors': info['config']['distance_cfg']['m_neighbors'],\n",
    "        'iterations': info['config']['distance_cfg']['iterations']\n",
    "    }\n",
    "    print(f\"\u2713 {preset:10s}: m_neighbors={results_presets[preset]['m_neighbors']:2d}, \"\n",
    "          f\"iterations={results_presets[preset]['iterations']}\")\n",
    "\n",
    "print(\"\\nPreset comparison:\")\n",
    "print(\"  fast:     Quick runs (fewer neighbors, 1 iteration)\")\n",
    "print(\"  balanced: Default (good trade-off)\")\n",
    "print(\"  robust:   More neighbors, 2 iterations (better quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics\n",
    "\n",
    "We'll compute distributional metrics to compare methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (same as basic_tabular)\n",
    "def wasserstein_1d_approx(x1, x2, w2=None, n_samples=5000):\n",
    "    if w2 is not None:\n",
    "        probs = w2 / w2.sum()\n",
    "        idx = np.random.choice(len(x2), size=n_samples, p=probs, replace=True)\n",
    "        x2_sampled = x2[idx]\n",
    "    else:\n",
    "        x2_sampled = x2\n",
    "    x1_sorted = np.sort(x1)\n",
    "    x2_sorted = np.sort(x2_sampled)\n",
    "    n = min(len(x1_sorted), len(x2_sorted))\n",
    "    quantiles = np.linspace(0, 1, n)\n",
    "    q1 = np.quantile(x1_sorted, quantiles)\n",
    "    q2 = np.quantile(x2_sorted, quantiles)\n",
    "    return np.abs(q1 - q2).mean()\n",
    "\n",
    "def weighted_mean(S, w):\n",
    "    return (S * w[:, None]).sum(axis=0)\n",
    "\n",
    "def weighted_cov(S, w):\n",
    "    mu = weighted_mean(S, w)\n",
    "    Xc = S - mu\n",
    "    return (Xc * w[:, None]).T @ Xc\n",
    "\n",
    "def compute_metrics(X_full, S, w, method_name):\n",
    "    mu_full = X_full.mean(axis=0)\n",
    "    cov_full = np.cov(X_full, rowvar=False)\n",
    "    mu_coreset = weighted_mean(S, w)\n",
    "    cov_coreset = weighted_cov(S, w)\n",
    "    mean_err = np.linalg.norm(mu_full - mu_coreset)\n",
    "    cov_err = np.linalg.norm(cov_full - cov_coreset, ord='fro')\n",
    "    \n",
    "    d = X_full.shape[1]\n",
    "    W1_dims = []\n",
    "    for dim in range(min(5, d)):  # First 5 features only\n",
    "        W1 = wasserstein_1d_approx(X_full[:, dim], S[:, dim], w)\n",
    "        W1_dims.append(W1)\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'mean_err_l2': mean_err,\n",
    "        'cov_err_fro': cov_err,\n",
    "        'W1_mean': np.mean(W1_dims),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all methods\n",
    "metrics_euclidean = compute_metrics(X, S_euclidean, w_euclidean, 'Euclidean')\n",
    "metrics_adaptive = compute_metrics(X, S_adaptive, w_adaptive, 'Adaptive')\n",
    "metrics_auto = compute_metrics(X, S_auto, w_auto, 'Auto')\n",
    "\n",
    "results_df = pd.DataFrame([metrics_euclidean, metrics_adaptive, metrics_auto])\n",
    "results_df = results_df.set_index('method')\n",
    "\n",
    "print(\"Distributional Metrics Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "print(\"\\nAdaptive Improvement over Euclidean:\")\n",
    "for metric in ['mean_err_l2', 'cov_err_fro', 'W1_mean']:\n",
    "    euclidean_val = metrics_euclidean[metric]\n",
    "    adaptive_val = metrics_adaptive[metric]\n",
    "    improvement = (1 - adaptive_val / euclidean_val) * 100\n",
    "    print(f\"{metric:20s}: {improvement:6.1f}% better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "Let's visualize the coresets in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to 2D\n",
    "if HAS_UMAP:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=RANDOM_STATE)\n",
    "else:\n",
    "    reducer = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "\n",
    "X_2d = reducer.fit_transform(X)\n",
    "S_euclidean_2d = reducer.transform(S_euclidean)\n",
    "S_adaptive_2d = reducer.transform(S_adaptive)\n",
    "\n",
    "# Find cluster labels for coreset points (use original S, not 2D projection)\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn_labels = NearestNeighbors(n_neighbors=1)\n",
    "nn_labels.fit(X)\n",
    "_, idx_euclidean = nn_labels.kneighbors(S_euclidean)\n",
    "_, idx_adaptive = nn_labels.kneighbors(S_adaptive)\n",
    "labels_euclidean = cluster_labels[idx_euclidean.flatten()]\n",
    "labels_adaptive = cluster_labels[idx_adaptive.flatten()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, S_2d, labels, title in zip(\n",
    "    axes, [S_euclidean_2d, S_adaptive_2d],\n",
    "    [labels_euclidean, labels_adaptive],\n",
    "    ['Euclidean Mode', 'Adaptive Mode']\n",
    "):\n",
    "    ax.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, \n",
    "              cmap='viridis', alpha=0.1, s=5)\n",
    "    scatter = ax.scatter(S_2d[:, 0], S_2d[:, 1], c=labels,\n",
    "                        cmap='viridis', s=50, alpha=0.8, \n",
    "                        edgecolors='black', linewidths=0.5)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### When to Use Adaptive Distances\n",
    "\n",
    "- **Elliptical clusters**: Adaptive handles non-spherical shapes better\n",
    "- **d \u2265 20**: Adaptive helps with medium-dimensional data\n",
    "- **d \u2265 50**: Auto mode triggers PCA reduction, then adaptive\n",
    "\n",
    "### Preset Guide\n",
    "\n",
    "- **fast**: Quick runs, fewer neighbors (24), 1 iteration\n",
    "- **balanced**: Default, good trade-off (32 neighbors, 1 iteration)\n",
    "- **robust**: Better quality, more neighbors (64), 2 iterations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try `high_dimensional.ipynb` for automatic PCA reduction\n",
    "- Try `label_aware_classification.ipynb` for supervised problems\n",
    "- See `docs/ADAPTIVE_DISTANCES_EXPLAINED.md` for technical details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}