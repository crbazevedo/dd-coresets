{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label-Aware DDC for Classification Problems\n",
    "\n",
    "This notebook demonstrates **label-aware DDC** for supervised learning problems. ",
    "We'll show how `fit_ddc_coreset_by_label` preserves class proportions while maintaining distributional fidelity.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- When to use label-aware DDC (supervised problems)\n",
    "- Preserving class proportions matters for classification\n",
    "- Impact on downstream model performance\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We'll use a **binary classification dataset** (Adult Census Income or synthetic) ",
    "to demonstrate label-aware DDC's advantage over global DDC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dd-coresets\n",
    "# For Google Colab: uncomment the line below\n",
    "# !pip install dd-coresets\n",
    "\n",
    "# For Kaggle: usually already available or use:\n",
    "# !pip install dd-coresets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from scipy.stats import wasserstein_distance, ks_2samp\n",
    "\n",
    "from dd_coresets import fit_ddc_coreset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set plotting style\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "We'll use the **Adult Census Income** dataset, a well-known binary classification dataset from the UCI Machine Learning Repository. This dataset contains demographic and employment information, with the goal of predicting whether income exceeds $50K/year.\n",
    "\n",
    "The dataset is publicly available and can be downloaded via `sklearn.datasets.fetch_openml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Adult Census Income dataset from OpenML\n",
    "print(\"Loading Adult Census Income dataset from OpenML...\")\n",
    "try:\n",
    "    # Try to load from cache first (faster)\n",
    "    adult = fetch_openml(\"adult\", version=2, as_frame=True, parser=\"pandas\")\n",
    "    print(f\"Dataset loaded: {adult.frame.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"This may take a few minutes on first download...\")\n",
    "    adult = fetch_openml(\"adult\", version=2, as_frame=True, parser=\"pandas\")\n",
    "\n",
    "# Extract features and target\n",
    "df = adult.frame.copy()\n",
    "\n",
    "# The target column is 'class' with values '<=50K' and '>50K'\n",
    "# Convert to binary: 0 for '<=50K', 1 for '>50K'\n",
    "if 'class' in df.columns:\n",
    "    df['target'] = (df['class'] == '>50K').astype(int)\n",
    "    df = df.drop(columns=['class'])\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[\"target\"].value_counts(normalize=True))\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "We'll select numeric features, handle missing values, and scale the data. DDC requires **preprocessed numerical features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features (exclude target)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col != 'target']\n",
    "\n",
    "print(f\"Selected {len(numeric_cols)} numeric features\")\n",
    "print(f\"Features: {numeric_cols}\")\n",
    "\n",
    "# Extract features and target\n",
    "X_raw = df[numeric_cols].copy()\n",
    "y_raw = df['target'].values\n",
    "\n",
    "# Handle missing values (simple mean imputation)\n",
    "if X_raw.isnull().sum().sum() > 0:\n",
    "    print(f\"\\nFound missing values. Imputing with mean...\")\n",
    "    X_raw = X_raw.fillna(X_raw.mean())\n",
    "else:\n",
    "    print(\"\\nNo missing values\")\n",
    "\n",
    "# Convert to NumPy array\n",
    "X_raw = X_raw.values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_raw.shape}\")\n",
    "print(f\"Target vector shape: {y_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for DDC, which uses Euclidean distances)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_raw)\n",
    "\n",
    "# Split into train/test (stratified to preserve label proportions)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_raw, \n",
    "    test_size=0.3, \n",
    "    stratify=y_raw, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nTraining label distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Class {label}: {count:,} ({count/len(y_train)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full-Data Baseline Model\n",
    "\n",
    "We'll train a logistic regression model on the **full training set** to establish a gold standard. This will be our baseline for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full training data\n",
    "lr_full = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=None  # No class balancing\n",
    ")\n",
    "\n",
    "lr_full.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_proba_full = lr_full.predict_proba(X_test)[:, 1]\n",
    "y_pred_full = lr_full.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "baseline_auc = roc_auc_score(y_test, y_pred_proba_full)\n",
    "baseline_brier = brier_score_loss(y_test, y_pred_proba_full)\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(\"Full-Data Baseline Metrics:\")\n",
    "print(f\"  ROC AUC:  {baseline_auc:.4f}\")\n",
    "print(f\"  Brier Score: {baseline_brier:.4f}\")\n",
    "print(f\"  Accuracy:    {baseline_accuracy:.4f}\")\n",
    "\n",
    "# Store for later comparison\n",
    "baseline_metrics = {\n",
    "    'method': 'Full Data',\n",
    "    'auc': baseline_auc,\n",
    "    'brier': baseline_brier,\n",
    "    'accuracy': baseline_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Subsets: Random and Stratified\n",
    "\n",
    "Before using DDC, let's establish simple baselines:\n",
    "\n",
    "- **Random subset**: Uniform sampling (may miss rare classes)\n",
    "- **Stratified subset**: Preserves class proportions (common practice in supervised learning)\n",
    "\n",
    "We'll use `k_reps = 1000` representatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_reps = 1000  # Number of representatives\n",
    "print(f\"Target coreset size: {k_reps} representatives ({k_reps/len(X_train)*100:.2f}% of training data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random subset\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random_indices = np.random.choice(len(X_train), size=k_reps, replace=False)\n",
    "X_random = X_train[random_indices]\n",
    "y_random = y_train[random_indices]\n",
    "w_random = np.ones(k_reps) / k_reps  # Uniform weights\n",
    "\n",
    "print(\"Random subset created\")\n",
    "print(f\"  Shape: {X_random.shape}\")\n",
    "print(f\"  Label distribution: {np.bincount(y_random) / len(y_random)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified subset (preserves class proportions)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Manual stratified sampling to get exactly k_reps\n",
    "strat_indices = []\n",
    "for class_label in np.unique(y_train):\n",
    "    class_mask = (y_train == class_label)\n",
    "    class_indices = np.where(class_mask)[0]\n",
    "    n_class = int(k_reps * np.sum(class_mask) / len(y_train))\n",
    "    selected = np.random.choice(class_indices, size=n_class, replace=False)\n",
    "    strat_indices.extend(selected)\n",
    "\n",
    "# If we don't have exactly k_reps, adjust\n",
    "if len(strat_indices) < k_reps:\n",
    "    remaining = k_reps - len(strat_indices)\n",
    "    remaining_indices = np.setdiff1d(np.arange(len(X_train)), strat_indices)\n",
    "    strat_indices.extend(np.random.choice(remaining_indices, size=remaining, replace=False))\n",
    "elif len(strat_indices) > k_reps:\n",
    "    strat_indices = np.random.choice(strat_indices, size=k_reps, replace=False)\n",
    "\n",
    "X_strat = X_train[strat_indices]\n",
    "y_strat = y_train[strat_indices]\n",
    "w_strat = np.ones(len(X_strat)) / len(X_strat)  # Uniform weights\n",
    "\n",
    "print(\"Stratified subset created\")\n",
    "print(f\"  Shape: {X_strat.shape}\")\n",
    "print(f\"  Label distribution: {np.bincount(y_strat) / len(y_strat)}\")\n",
    "print(f\"  Original label distribution: {np.bincount(y_train) / len(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Label-Aware DDC Coreset\n",
    "\n",
    "To preserve class proportions while still benefiting from DDC's distribution-preserving properties, we apply DDC **separately within each class**. This:\n",
    "\n",
    "1. Preserves label proportions by design\n",
    "2. Maintains density\u2013diversity structure **within each class**\n",
    "3. Still provides weighted representatives that approximate the full distribution\n",
    "\n",
    "**Key insight**: By applying DDC separately to each class, we ensure that the coreset maintains the original class balance while still capturing the distributional structure within each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label-aware DDC: apply DDC separately to each class\n",
    "S_labelaware_list = []\n",
    "w_labelaware_list = []\n",
    "y_labelaware_list = []\n",
    "\n",
    "for class_label in np.unique(y_train):\n",
    "    # Extract data for this class\n",
    "    class_mask = (y_train == class_label)\n",
    "    X_class = X_train[class_mask]\n",
    "    \n",
    "    # Compute class proportion\n",
    "    p_class = np.sum(class_mask) / len(y_train)\n",
    "    \n",
    "    # Allocate representatives proportionally\n",
    "    k_class = max(1, int(round(k_reps * p_class)))\n",
    "    \n",
    "    print(f\"\\nClass {class_label}: {np.sum(class_mask):,} samples ({p_class:.2%})\")\n",
    "    print(f\"  Allocating {k_class} representatives...\")\n",
    "    \n",
    "    # Fit DDC on this class\n",
    "    # Use larger n0 for better density estimation when class is large enough\n",
    "    n0_class = min(20_000, len(X_class))\n",
    "    \n",
    "    # Optimized parameters (from grid search to minimize joint distribution errors)\n",
    "    # These parameters were found to minimize covariance and correlation errors\n",
    "    alpha_opt = 0.2  # Lower alpha favors diversity (better coverage)\n",
    "    gamma_opt = 1.5  # Higher gamma for smoother weight assignments\n",
    "    m_neighbors_opt = 16  # Fewer neighbors for faster computation, still adequate\n",
    "    refine_iters_opt = 2  # More refinement iterations for better quality\n",
    "    \n",
    "    # Adjust m_neighbors for very small classes\n",
    "    m_neighbors_class = max(5, min(m_neighbors_opt, len(X_class) // 10))\n",
    "    \n",
    "    S_class, w_class, info_class = fit_ddc_coreset(\n",
    "        X_class,\n",
    "        k=k_class,\n",
    "        n0=n0_class,\n",
    "        alpha=alpha_opt,\n",
    "        m_neighbors=m_neighbors_class,\n",
    "        gamma=gamma_opt,\n",
    "        refine_iters=refine_iters_opt,\n",
    "        reweight_full=True,  # Important: reweight on full class data\n",
    "        random_state=RANDOM_STATE + class_label,  # Different seed per class\n",
    "    )\n",
    "    \n",
    "    # Scale weights by class proportion to preserve global distribution\n",
    "    w_class_scaled = w_class * p_class\n",
    "    \n",
    "    S_labelaware_list.append(S_class)\n",
    "    w_labelaware_list.append(w_class_scaled)  # Already scaled by proportion\n",
    "    y_labelaware_list.append(np.full(len(S_class), class_label))\n",
    "\n",
    "# Concatenate all classes\n",
    "S_labelaware = np.vstack(S_labelaware_list)\n",
    "w_labelaware = np.concatenate(w_labelaware_list)\n",
    "y_labelaware = np.concatenate(y_labelaware_list)\n",
    "\n",
    "# Renormalize weights (they should already sum close to 1, but ensure it)\n",
    "w_labelaware = w_labelaware / w_labelaware.sum()\n",
    "\n",
    "print(f\"\\nLabel-aware DDC coreset created: {S_labelaware.shape}\")\n",
    "print(f\"  Weights sum: {w_labelaware.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify label proportions are preserved\n",
    "print(\"Label Distribution Comparison:\")\n",
    "print(f\"  Original training set:\")\n",
    "orig_props = np.bincount(y_train) / len(y_train)\n",
    "for label, prop in enumerate(orig_props):\n",
    "    print(f\"    Class {label}: {prop:.4f}\")\n",
    "\n",
    "print(f\"\\n  Label-aware DDC coreset:\")\n",
    "labelaware_props = np.bincount(y_labelaware) / len(y_labelaware)\n",
    "for label, prop in enumerate(labelaware_props):\n",
    "    print(f\"    Class {label}: {prop:.4f}\")\n",
    "\n",
    "print(f\"\\nClass proportion preservation:\")\n",
    "for label in range(len(orig_props)):\n",
    "    diff = abs(labelaware_props[label] - orig_props[label])\n",
    "    print(f\"    Class {label}: {diff:.6f} difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution Comparison\n",
    "\n",
    "Let's compare how well each subset/coreset preserves the **marginal distributions** of the original training data. We'll use:\n",
    "\n",
    "- **Wasserstein-1 distance**: Measures how much we need to \"move\" probability mass to match distributions\n",
    "- **Kolmogorov-Smirnov statistic**: Measures the maximum difference between cumulative distribution functions\n",
    "\n",
    "For DDC coresets, we'll use the **weights** to compute weighted distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few features for comparison (e.g., first 5 or high-variance features)\n",
    "feature_indices = list(range(min(5, X_train.shape[1])))\n",
    "feature_names = [f\"Feature {i}\" for i in feature_indices]\n",
    "\n",
    "print(f\"Comparing distributions for {len(feature_indices)} features:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wasserstein_weighted(source_data, target_data, source_weights=None, target_weights=None):\n",
    "    \"\"\"Compute Wasserstein-1 distance between weighted distributions.\"\"\"\n",
    "    if source_weights is None:\n",
    "        source_weights = np.ones(len(source_data)) / len(source_data)\n",
    "    if target_weights is None:\n",
    "        target_weights = np.ones(len(target_data)) / len(target_data)\n",
    "    \n",
    "    # Sort by value for computing Wasserstein distance\n",
    "    source_sorted_idx = np.argsort(source_data)\n",
    "    target_sorted_idx = np.argsort(target_data)\n",
    "    \n",
    "    source_sorted = source_data[source_sorted_idx]\n",
    "    target_sorted = target_data[target_sorted_idx]\n",
    "    \n",
    "    source_weights_sorted = source_weights[source_sorted_idx]\n",
    "    target_weights_sorted = target_weights[target_sorted_idx]\n",
    "    \n",
    "    # Compute cumulative distributions\n",
    "    source_cdf = np.cumsum(source_weights_sorted)\n",
    "    target_cdf = np.cumsum(target_weights_sorted)\n",
    "    \n",
    "    # Interpolate to common grid\n",
    "    all_values = np.unique(np.concatenate([source_sorted, target_sorted]))\n",
    "    all_values = np.sort(all_values)\n",
    "    \n",
    "    source_cdf_interp = np.interp(all_values, source_sorted, source_cdf, left=0, right=1)\n",
    "    target_cdf_interp = np.interp(all_values, target_sorted, target_cdf, left=0, right=1)\n",
    "    \n",
    "    # Wasserstein-1 is the integral of |CDF_diff|\n",
    "    w1 = np.trapz(np.abs(source_cdf_interp - target_cdf_interp), all_values)\n",
    "    \n",
    "    return w1\n",
    "\n",
    "def compute_ks_weighted(source_data, target_data, source_weights=None, target_weights=None, n_samples=10000):\n",
    "    \"\"\"Approximate KS statistic for weighted distributions by sampling.\"\"\"\n",
    "    if source_weights is None:\n",
    "        source_weights = np.ones(len(source_data)) / len(source_data)\n",
    "    if target_weights is None:\n",
    "        target_weights = np.ones(len(target_data)) / len(target_data)\n",
    "    \n",
    "    # Sample from weighted distributions\n",
    "    source_samples = np.random.choice(\n",
    "        source_data, size=n_samples, p=source_weights / source_weights.sum(), replace=True\n",
    "    )\n",
    "    target_samples = np.random.choice(\n",
    "        target_data, size=n_samples, p=target_weights / target_weights.sum(), replace=True\n",
    "    )\n",
    "    \n",
    "    # Compute KS statistic\n",
    "    ks_stat, _ = ks_2samp(source_samples, target_samples)\n",
    "    \n",
    "    return ks_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions for each feature\n",
    "results = []\n",
    "\n",
    "for feat_idx, feat_name in zip(feature_indices, feature_names):\n",
    "    # Full training data (reference)\n",
    "    X_train_feat = X_train[:, feat_idx]\n",
    "    \n",
    "    # Random subset\n",
    "    X_random_feat = X_random[:, feat_idx]\n",
    "    w1_random = compute_wasserstein_weighted(X_train_feat, X_random_feat)\n",
    "    ks_random = compute_ks_weighted(X_train_feat, X_random_feat)\n",
    "    \n",
    "    # Stratified subset\n",
    "    X_strat_feat = X_strat[:, feat_idx]\n",
    "    w1_strat = compute_wasserstein_weighted(X_train_feat, X_strat_feat)\n",
    "    ks_strat = compute_ks_weighted(X_train_feat, X_strat_feat)\n",
    "    \n",
    "    # Label-aware DDC\n",
    "    S_labelaware_feat = S_labelaware[:, feat_idx]\n",
    "    w1_labelaware = compute_wasserstein_weighted(X_train_feat, S_labelaware_feat, target_weights=w_labelaware)\n",
    "    ks_labelaware = compute_ks_weighted(X_train_feat, S_labelaware_feat, target_weights=w_labelaware)\n",
    "    \n",
    "    results.append({\n",
    "        'feature': feat_name,\n",
    "        'W1_random': w1_random,\n",
    "        'W1_strat': w1_strat,\n",
    "        'W1_labelaware_ddc': w1_labelaware,\n",
    "        'KS_random': ks_random,\n",
    "        'KS_strat': ks_strat,\n",
    "        'KS_labelaware_ddc': ks_labelaware,\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "dist_results_df = pd.DataFrame(results)\n",
    "print(\"Distribution Preservation Metrics:\")\n",
    "print(\"\\nWasserstein-1 Distance (lower is better):\")\n",
    "print(dist_results_df[['feature', 'W1_random', 'W1_strat', 'W1_labelaware_ddc']].to_string(index=False))\n",
    "print(\"\\nKolmogorov-Smirnov Statistic (lower is better):\")\n",
    "print(dist_results_df[['feature', 'KS_random', 'KS_strat', 'KS_labelaware_ddc']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average metrics across features\n",
    "avg_metrics = {\n",
    "    'Method': ['Random', 'Stratified', 'Label-aware DDC'],\n",
    "    'Avg W1': [\n",
    "        dist_results_df['W1_random'].mean(),\n",
    "        dist_results_df['W1_strat'].mean(),\n",
    "        dist_results_df['W1_labelaware_ddc'].mean(),\n",
    "    ],\n",
    "    'Avg KS': [\n",
    "        dist_results_df['KS_random'].mean(),\n",
    "        dist_results_df['KS_strat'].mean(),\n",
    "        dist_results_df['KS_labelaware_ddc'].mean(),\n",
    "    ]\n",
    "}\n",
    "\n",
    "avg_df = pd.DataFrame(avg_metrics)\n",
    "print(\"Average Distribution Preservation (across features):\")\n",
    "print(avg_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Joint Distribution Comparison\n",
    "\n",
    "While marginal distributions are important, for classification tasks we also need to preserve the **joint distribution** structure (covariances, correlations). Let's compute:\n",
    "\n",
    "- **Mean Error (L2)**: Difference in mean vectors\n",
    "- **Covariance Error (Frobenius)**: Difference in covariance matrices\n",
    "- **Correlation Error (Frobenius)**: Difference in correlation matrices\n",
    "- **Maximum Mean Discrepancy (MMD)**: Kernel-based distance between distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for joint distribution metrics\n",
    "def weighted_mean(S, w):\n",
    "    \"\"\"Compute weighted mean.\"\"\"\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    return (S * w[:, None]).sum(axis=0)\n",
    "\n",
    "def weighted_cov(S, w):\n",
    "    \"\"\"Compute weighted covariance matrix.\"\"\"\n",
    "    S = np.asarray(S, dtype=float)\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    mu = weighted_mean(S, w)\n",
    "    Xc = S - mu\n",
    "    cov = (Xc * w[:, None]).T @ Xc\n",
    "    return cov\n",
    "\n",
    "def corr_from_cov(cov):\n",
    "    \"\"\"Compute correlation matrix from covariance matrix.\"\"\"\n",
    "    cov = np.asarray(cov, dtype=float)\n",
    "    std = np.sqrt(np.clip(np.diag(cov), 1e-12, None))\n",
    "    inv_std = 1.0 / std\n",
    "    C = cov * inv_std[:, None] * inv_std[None, :]\n",
    "    return C\n",
    "\n",
    "def compute_mmd(X, Y, w_Y=None, kernel='rbf', gamma=None, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) between X and weighted Y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n, d) array\n",
    "        Full dataset\n",
    "    Y : (k, d) array\n",
    "        Coreset representatives\n",
    "    w_Y : (k,) array, optional\n",
    "        Weights for Y (default: uniform)\n",
    "    kernel : str\n",
    "        Kernel type ('rbf')\n",
    "    gamma : float, optional\n",
    "        RBF kernel bandwidth (default: median pairwise distance)\n",
    "    n_samples : int\n",
    "        Number of samples for approximation\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    Y = np.asarray(Y, dtype=float)\n",
    "    \n",
    "    if w_Y is None:\n",
    "        w_Y = np.ones(len(Y)) / len(Y)\n",
    "    else:\n",
    "        w_Y = np.asarray(w_Y, dtype=float)\n",
    "        w_Y = w_Y / w_Y.sum()\n",
    "    \n",
    "    # Sample from X (uniform) and Y (weighted)\n",
    "    n_sample = min(n_samples, len(X))\n",
    "    idx_x = np.random.choice(len(X), size=n_sample, replace=False)\n",
    "    X_sample = X[idx_x]\n",
    "    \n",
    "    idx_y = np.random.choice(len(Y), size=n_sample, p=w_Y, replace=True)\n",
    "    Y_sample = Y[idx_y]\n",
    "    \n",
    "    # Compute pairwise distances for gamma estimation\n",
    "    if gamma is None:\n",
    "        all_data = np.vstack([X_sample, Y_sample])\n",
    "        pairwise_dists = np.sqrt(((all_data[:, None, :] - all_data[None, :, :]) ** 2).sum(axis=2))\n",
    "        gamma = 1.0 / np.median(pairwise_dists[pairwise_dists > 0])\n",
    "    \n",
    "    # RBF kernel\n",
    "    def rbf_kernel(X1, X2):\n",
    "        dists_sq = ((X1[:, None, :] - X2[None, :, :]) ** 2).sum(axis=2)\n",
    "        return np.exp(-gamma * dists_sq)\n",
    "    \n",
    "    # MMD^2 = E[k(x,x')] - 2*E[k(x,y)] + E[k(y,y')]\n",
    "    K_XX = rbf_kernel(X_sample, X_sample)\n",
    "    K_YY = rbf_kernel(Y_sample, Y_sample)\n",
    "    K_XY = rbf_kernel(X_sample, Y_sample)\n",
    "    \n",
    "    mmd_sq = K_XX.mean() - 2 * K_XY.mean() + K_YY.mean()\n",
    "    return np.sqrt(max(0, mmd_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute joint distribution metrics for each method\n",
    "joint_metrics = []\n",
    "\n",
    "# Reference: Full training data\n",
    "mu_full = X_train.mean(axis=0)\n",
    "cov_full = np.cov(X_train, rowvar=False)\n",
    "corr_full = corr_from_cov(cov_full)\n",
    "\n",
    "# Random subset\n",
    "mu_random = X_random.mean(axis=0)\n",
    "cov_random = np.cov(X_random, rowvar=False)\n",
    "corr_random = corr_from_cov(cov_random)\n",
    "mean_err_random = np.linalg.norm(mu_full - mu_random)\n",
    "cov_err_random = np.linalg.norm(cov_full - cov_random, ord='fro')\n",
    "corr_err_random = np.linalg.norm(corr_full - corr_random, ord='fro')\n",
    "mmd_random = compute_mmd(X_train, X_random)\n",
    "\n",
    "joint_metrics.append({\n",
    "    'method': 'Random',\n",
    "    'mean_err_l2': mean_err_random,\n",
    "    'cov_err_fro': cov_err_random,\n",
    "    'corr_err_fro': corr_err_random,\n",
    "    'mmd': mmd_random,\n",
    "})\n",
    "\n",
    "# Stratified subset\n",
    "mu_strat = X_strat.mean(axis=0)\n",
    "cov_strat = np.cov(X_strat, rowvar=False)\n",
    "corr_strat = corr_from_cov(cov_strat)\n",
    "mean_err_strat = np.linalg.norm(mu_full - mu_strat)\n",
    "cov_err_strat = np.linalg.norm(cov_full - cov_strat, ord='fro')\n",
    "corr_err_strat = np.linalg.norm(corr_full - corr_strat, ord='fro')\n",
    "mmd_strat = compute_mmd(X_train, X_strat)\n",
    "\n",
    "joint_metrics.append({\n",
    "    'method': 'Stratified',\n",
    "    'mean_err_l2': mean_err_strat,\n",
    "    'cov_err_fro': cov_err_strat,\n",
    "    'corr_err_fro': corr_err_strat,\n",
    "    'mmd': mmd_strat,\n",
    "})\n",
    "\n",
    "# Label-aware DDC (use weights)\n",
    "mu_labelaware = weighted_mean(S_labelaware, w_labelaware)\n",
    "cov_labelaware = weighted_cov(S_labelaware, w_labelaware)\n",
    "corr_labelaware = corr_from_cov(cov_labelaware)\n",
    "mean_err_labelaware = np.linalg.norm(mu_full - mu_labelaware)\n",
    "cov_err_labelaware = np.linalg.norm(cov_full - cov_labelaware, ord='fro')\n",
    "corr_err_labelaware = np.linalg.norm(corr_full - corr_labelaware, ord='fro')\n",
    "mmd_labelaware = compute_mmd(X_train, S_labelaware, w_Y=w_labelaware)\n",
    "\n",
    "joint_metrics.append({\n",
    "    'method': 'Label-aware DDC',\n",
    "    'mean_err_l2': mean_err_labelaware,\n",
    "    'cov_err_fro': cov_err_labelaware,\n",
    "    'corr_err_fro': corr_err_labelaware,\n",
    "    'mmd': mmd_labelaware,\n",
    "})\n",
    "\n",
    "# Create DataFrame\n",
    "joint_metrics_df = pd.DataFrame(joint_metrics)\n",
    "print(\"Joint Distribution Preservation Metrics:\")\n",
    "print(\"\\n\" + joint_metrics_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  Mean Error (L2): Lower is better (0 = identical means)\")\n",
    "print(\"  Covariance Error (Frobenius): Lower is better (0 = identical covariances)\")\n",
    "print(\"  Correlation Error (Frobenius): Lower is better (0 = identical correlations)\")\n",
    "print(\"  MMD: Lower is better (0 = identical distributions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Downstream Model Comparison\n",
    "\n",
    "Now let's train logistic regression models on each subset/coreset and evaluate on the **same test set**. This shows the **practical impact** of distribution preservation on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on each subset/coreset\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "# 1. Random subset\n",
    "lr_random = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight=None)\n",
    "lr_random.fit(X_random, y_random)\n",
    "models['Random'] = lr_random\n",
    "predictions['Random'] = lr_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2. Stratified subset\n",
    "lr_strat = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight=None)\n",
    "lr_strat.fit(X_strat, y_strat)\n",
    "models['Stratified'] = lr_strat\n",
    "predictions['Stratified'] = lr_strat.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 3. Label-aware DDC coreset (use weights)\n",
    "lr_labelaware = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE, class_weight=None)\n",
    "sample_weights_labelaware = w_labelaware * len(X_train)  # Scale weights to approximate sample counts\n",
    "lr_labelaware.fit(S_labelaware, y_labelaware, sample_weight=sample_weights_labelaware)\n",
    "models['Label-aware DDC'] = lr_labelaware\n",
    "predictions['Label-aware DDC'] = lr_labelaware.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"All models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "model_results = []\n",
    "\n",
    "# Full data baseline (already computed)\n",
    "model_results.append(baseline_metrics)\n",
    "\n",
    "# Evaluate other methods\n",
    "for method_name, y_pred_proba in predictions.items():\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    brier = brier_score_loss(y_test, y_pred_proba)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    model_results.append({\n",
    "        'method': method_name,\n",
    "        'auc': auc,\n",
    "        'brier': brier,\n",
    "        'accuracy': accuracy\n",
    "    })\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(model_results)\n",
    "comparison_df['auc_diff'] = comparison_df['auc'] - baseline_auc\n",
    "comparison_df['brier_diff'] = comparison_df['brier'] - baseline_brier\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nDeviation from Full-Data Baseline:\")\n",
    "for method in ['Random', 'Stratified', 'Label-aware DDC']:\n",
    "    row = comparison_df[comparison_df['method'] == method].iloc[0]\n",
    "    print(f\"  {method:20s}: AUC {row['auc_diff']:+.4f}, Brier {row['brier_diff']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizations\n",
    "\n",
    "Let's visualize the distribution preservation and spatial coverage of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot marginal distributions for a couple of features\n",
    "n_features_to_plot = min(2, len(feature_indices))\n",
    "\n",
    "fig, axes = plt.subplots(1, n_features_to_plot, figsize=(6 * n_features_to_plot, 5))\n",
    "if n_features_to_plot == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for plot_idx, feat_idx in enumerate(feature_indices[:n_features_to_plot]):\n",
    "    ax = axes[plot_idx]\n",
    "    \n",
    "    # Full training data (reference)\n",
    "    ax.hist(X_train[:, feat_idx], bins=50, density=True, alpha=0.3, \n",
    "            label='Full Data', color='gray', edgecolor='black')\n",
    "    \n",
    "    # Random subset\n",
    "    ax.hist(X_random[:, feat_idx], bins=30, density=True, alpha=0.5, \n",
    "            label='Random', color='blue', histtype='step', linewidth=2)\n",
    "    \n",
    "    # Stratified subset\n",
    "    ax.hist(X_strat[:, feat_idx], bins=30, density=True, alpha=0.5, \n",
    "            label='Stratified', color='green', histtype='step', linewidth=2)\n",
    "    \n",
    "    # Label-aware DDC (weighted)\n",
    "    ax.hist(S_labelaware[:, feat_idx], bins=30, weights=w_labelaware, density=True, \n",
    "            label='Label-aware DDC', color='orange', histtype='step', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel(f'Feature {feat_idx}')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Marginal Distribution: Feature {feat_idx}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D PCA projection to visualize spatial coverage\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_train_2d = pca.fit_transform(X_train)\n",
    "\n",
    "# Project subsets\n",
    "X_random_2d = pca.transform(X_random)\n",
    "X_strat_2d = pca.transform(X_strat)\n",
    "S_labelaware_2d = pca.transform(S_labelaware)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "methods_2d = [\n",
    "    ('Random', X_random_2d, y_random, None, 'blue'),\n",
    "    ('Stratified', X_strat_2d, y_strat, None, 'green'),\n",
    "    ('Label-aware DDC', S_labelaware_2d, y_labelaware, w_labelaware, 'orange'),\n",
    "]\n",
    "\n",
    "for ax, (method_name, subset_2d, subset_y, subset_w, color) in zip(axes, methods_2d):\n",
    "    # Background: full data (low alpha)\n",
    "    ax.scatter(X_train_2d[:, 0], X_train_2d[:, 1], \n",
    "              c=y_train, cmap='RdYlBu', alpha=0.1, s=1, label='Full Data')\n",
    "    \n",
    "    # Overlay: representatives\n",
    "    if subset_w is not None:\n",
    "        # Size proportional to weight\n",
    "        sizes = 200 * (subset_w / subset_w.max())\n",
    "    else:\n",
    "        sizes = 50\n",
    "    \n",
    "    ax.scatter(subset_2d[:, 0], subset_2d[:, 1], \n",
    "              c=subset_y, cmap='RdYlBu', s=sizes, \n",
    "              edgecolors='black', linewidth=0.5, alpha=0.8, label=method_name)\n",
    "    \n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_title(f'{method_name} (n={len(subset_2d)})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# Full data baseline\n",
    "fpr_full, tpr_full, _ = roc_curve(y_test, y_pred_proba_full)\n",
    "ax.plot(fpr_full, tpr_full, label=f'Full Data (AUC={baseline_auc:.4f})', \n",
    "        linewidth=2, color='black', linestyle='--')\n",
    "\n",
    "# Other methods\n",
    "for method_name, y_pred_proba in predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    ax.plot(fpr, tpr, label=f'{method_name} (AUC={auc:.4f})', linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random Classifier')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Discussion and Takeaways\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "1. **Label-aware DDC preserves class balance**: By applying DDC separately within each class, we maintain label proportions while still benefiting from density\u2013diversity selection **within each class**. This approach:\n",
    "   - Preserves class proportions by design\n",
    "   - Maintains distributional fidelity within each class\n",
    "   - Typically performs closer to the full-data baseline than naive random sampling\n",
    "\n",
    "2. **Distribution preservation matters**: Methods that better preserve marginal distributions (measured by Wasserstein-1 and KS statistics) tend to produce models that perform closer to the full-data baseline.\n",
    "\n",
    "3. **Weights are essential**: DDC coresets are **weighted sets**, not just point sets. The weights allow us to approximate the full distribution from a small number of representatives.\n",
    "\n",
    "4. **Parameter tuning matters**: For label-aware DDC, we adjust parameters (alpha, m_neighbors, refine_iters) based on class size to ensure good coverage and distribution preservation.\n",
    "\n",
    "### When to Use Label-Aware DDC?\n",
    "\n",
    "**Use Label-aware DDC when:**\n",
    "- You're working on a **supervised learning** problem\n",
    "- Label proportions matter (e.g., imbalanced classification)\n",
    "- You want both distribution preservation AND label balance\n",
    "- You need a small, interpretable subset for model prototyping\n",
    "- You want to compress large datasets while maintaining class structure\n",
    "\n",
    "**Use Random/Stratified sampling when:**\n",
    "- You need a simple baseline for comparison\n",
    "- You don't need distribution preservation\n",
    "- Computational resources are extremely limited\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "DDC coresets provide a principled way to compress large datasets while preserving distributional properties. For supervised learning tasks, **label-aware DDC** is the recommended approach as it combines the benefits of distribution preservation with label balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- GitHub: https://github.com/crbazevedo/dd-coresets\n",
    "- PyPI: https://pypi.org/project/dd-coresets/\n",
    "- Documentation: See the main README for API details and more examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}