{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tabular Data: DDC vs Random vs Stratified\n",
    "\n",
    "This notebook demonstrates the basic usage of `dd-coresets` on simple tabular data. ",
    "We'll compare **Density-Diversity Coresets (DDC)** with **Random** and **Stratified** sampling.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to install and use `dd-coresets`\n",
    "- Basic API: `fit_ddc_coreset`, `fit_random_coreset`, `fit_stratified_coreset`\n",
    "- Understanding distributional metrics (Mean, Covariance, Wasserstein-1)\n",
    "- When DDC is better than Random (clustered data)\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We'll use a simple **Gaussian mixture** with 3 clusters and 8 features. ",
    "This structure is common in real-world data and demonstrates DDC's advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dd-coresets (uncomment if needed)\n",
    "# !pip install dd-coresets\n",
    "\n",
    "# For Kaggle/Colab, you may need:\n",
    "# !pip install dd-coresets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Try importing UMAP, fallback to PCA if not available\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"UMAP not available, using PCA for visualization\")\n",
    "\n",
    "from dd_coresets import (\n",
    "    fit_ddc_coreset,\n",
    "    fit_random_coreset,\n",
    "    fit_stratified_coreset,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Gaussian Mixtures?\n",
    "\n",
    "Gaussian mixtures with well-separated clusters are ideal for demonstrating DDC because:\n",
    "\n",
    "- **Clustered structure**: DDC excels when data has clear modes (clusters)\n",
    "- **Spatial coverage**: DDC guarantees all clusters are represented, even small ones\n",
    "- **Distribution preservation**: The weighted coreset preserves both the location (mean) and shape (covariance) of each cluster\n",
    "\n",
    "**Conceptual note**: In high dimensions, k-NN density estimation works well when clusters are separated. Points in dense regions (clusters) have many close neighbors, leading to high density estimates. DDC uses these estimates to prioritize important regions while ensuring diversity (spatial coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Gaussian Mixtures?\n",
    "\n",
    "Gaussian mixtures with well-separated clusters are ideal for demonstrating DDC because:\n",
    "\n",
    "- **Clustered structure**: DDC excels when data has clear modes (clusters)\n",
    "- **Spatial coverage**: DDC guarantees all clusters are represented, even small ones\n",
    "- **Distribution preservation**: The weighted coreset preserves both the location (mean) and shape (covariance) of each cluster\n",
    "\n",
    "**Conceptual note**: In high dimensions, k-NN density estimation works well when clusters are separated. Points in dense regions (clusters) have many close neighbors, leading to high density estimates. DDC uses these estimates to prioritize important regions while ensuring diversity (spatial coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Dataset\n",
    "\n",
    "We'll create a Gaussian mixture with **3 clusters** and **8 features**. ",
    "The clusters are well-separated, which is where DDC typically excels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gaussian mixture\n",
    "n_samples = 10000\n",
    "n_features = 8\n",
    "n_clusters = 3\n",
    "\n",
    "X, cluster_labels = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    centers=n_clusters,\n",
    "    cluster_std=1.5,\n",
    "    center_box=(-10, 10),\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Cluster sizes: {np.bincount(cluster_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Gaussian Mixtures?\n",
    "\n",
    "Gaussian mixtures with well-separated clusters are ideal for demonstrating DDC because:\n",
    "\n",
    "- **Clustered structure**: DDC excels when data has clear modes (clusters)\n",
    "- **Spatial coverage**: DDC guarantees all clusters are represented, even small ones\n",
    "- **Distribution preservation**: The weighted coreset preserves both the location (mean) and shape (covariance) of each cluster\n",
    "\n",
    "**Conceptual note**: In high dimensions, k-NN density estimation works well when clusters are separated. Points in dense regions (clusters) have many close neighbors, leading to high density estimates. DDC uses these estimates to prioritize important regions while ensuring diversity (spatial coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Gaussian Mixtures?\n",
    "\n",
    "Gaussian mixtures with well-separated clusters are ideal for demonstrating DDC because:\n",
    "\n",
    "- **Clustered structure**: DDC excels when data has clear modes (clusters)\n",
    "- **Spatial coverage**: DDC guarantees all clusters are represented, even small ones\n",
    "- **Distribution preservation**: The weighted coreset preserves both the location (mean) and shape (covariance) of each cluster\n",
    "\n",
    "**Conceptual note**: In high dimensions, k-NN density estimation works well when clusters are separated. Points in dense regions (clusters) have many close neighbors, leading to high density estimates. DDC uses these estimates to prioritize important regions while ensuring diversity (spatial coverage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Original Data\n",
    "\n",
    "Let's visualize the data in 2D using UMAP (or PCA if UMAP is not available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to 2D for visualization",
    "if HAS_UMAP:",
    "    reducer = umap.UMAP(n_components=2, random_state=RANDOM_STATE)",
    "    X_2d = reducer.fit_transform(X)",
    "else:",
    "    reducer = PCA(n_components=2, random_state=RANDOM_STATE)",
    "    X_2d = reducer.fit_transform(X)",
    "",
    "plt.figure(figsize=(10, 6))",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, ",
    "                      cmap='viridis', alpha=0.5, s=10)",
    "plt.colorbar(scatter, label='Cluster')",
    "plt.title('Original Data (2D Projection)', fontsize=14, fontweight='bold')",
    "plt.xlabel('Component 1')",
    "plt.ylabel('Component 2')",
    "plt.tight_layout()",
    "",
    "# Save figure",
    "import os",
    "os.makedirs('images/tutorials/basic_tabular', exist_ok=True)",
    "plt.savefig('images/tutorials/basic_tabular/original_data_2d.png', dpi=150, bbox_inches='tight')",
    "",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2D projection of original Gaussian mixture data with 3 clusters](images/tutorials/basic_tabular/original_data_2d.png)\n",
    "\n",
    "*Original dataset projected to 2D using UMAP (or PCA). Colors represent different clusters.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fit Coresets\n",
    "\n",
    "Now we'll create coresets using three methods:\n",
    "\n",
    "1. **DDC**: Density-Diversity Coreset (unsupervised)\n",
    "2. **Random**: Uniform random sampling\n",
    "3. **Stratified**: Stratified sampling by cluster\n",
    "\n",
    "We'll use `k=200` representatives (2% of the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200  # Number of representatives\n",
    "\n",
    "print(\"Fitting coresets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# DDC (using default mode='euclidean', simplest preset)\n",
    "S_ddc, w_ddc, info_ddc = fit_ddc_coreset(\n",
    "    X, k=k, mode='euclidean', random_state=RANDOM_STATE\n",
    ")\n",
    "print(f\"\u2713 DDC: {len(S_ddc)} representatives\")\n",
    "print(f\"  Pipeline: {info_ddc['pipeline']}\")\n",
    "\n",
    "# Random\n",
    "S_random, w_random, info_random = fit_random_coreset(\n",
    "    X, k=k, random_state=RANDOM_STATE + 1\n",
    ")\n",
    "print(f\"\u2713 Random: {len(S_random)} representatives\")\n",
    "\n",
    "# Stratified (by cluster)\n",
    "S_strat, w_strat, info_strat = fit_stratified_coreset(\n",
    "    X, strata=cluster_labels, k=k, random_state=RANDOM_STATE + 2\n",
    ")\n",
    "print(f\"\u2713 Stratified: {len(S_strat)} representatives\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens During DDC Fitting?\n",
    "\n",
    "The `fit_ddc_coreset` function performs three main steps:\n",
    "\n",
    "1. **Density Estimation**: Estimates local density for each point using k-nearest neighbors. Points in dense regions (clusters) get high density scores.\n",
    "\n",
    "2. **Greedy Selection**: Iteratively selects points that balance high density (important regions) with diversity (spatial coverage). The `alpha` parameter (default 0.3) controls this trade-off.\n",
    "\n",
    "3. **Weight Assignment**: Assigns weights to selected points using soft assignments. A point with weight 0.1 \"stands for\" 10% of the original data in that region.\n",
    "\n",
    "**Why weights matter**: Unlike simple sampling where each point represents 1/n of the data, weights allow a small coreset to accurately represent the full distribution. This is similar to how a histogram uses bin counts, but DDC uses actual data points with weights.\n",
    "\n",
    "See [Algorithm Overview](../concepts/algorithm.md) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Metrics\n",
    "\n",
    "We'll compute distributional metrics to compare how well each coreset preserves the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def wasserstein_1d_approx(x1, x2, w2=None, n_samples=5000):\n    \"\"\"Approximate Wasserstein-1 distance for 1D distributions.\"\"\"\n    if w2 is not None:\n        probs = w2 / w2.sum()\n        idx = np.random.choice(len(x2), size=n_samples, p=probs, replace=True)\n        x2_sampled = x2[idx]\n    else:\n        x2_sampled = x2\n    \n    x1_sorted = np.sort(x1)\n    x2_sorted = np.sort(x2_sampled)\n    n = min(len(x1_sorted), len(x2_sorted))\n    quantiles = np.linspace(0, 1, n)\n    q1 = np.quantile(x1_sorted, quantiles)\n    q2 = np.quantile(x2_sorted, quantiles)\n    return np.abs(q1 - q2).mean()\n\n\ndef ks_1d_approx(x1, x2, w2=None, n_grid=512):\n    \"\"\"Approximate KS statistic for 1D distributions.\"\"\"\n    x_min = min(x1.min(), x2.min())\n    x_max = max(x1.max(), x2.max())\n    grid = np.linspace(x_min, x_max, n_grid)\n    \n    F_X = np.array([np.mean(x1 <= x) for x in grid])\n    \n    if w2 is not None:\n        F_S = np.array([np.sum(w2[x2 <= x]) for x in grid])\n    else:\n        F_S = np.array([np.mean(x2 <= x) for x in grid])\n    \n    return float(np.max(np.abs(F_X - F_S)))\n\n\ndef weighted_mean(S, w):\n    \"\"\"Compute weighted mean.\"\"\"\n    return (S * w[:, None]).sum(axis=0)\n\n\ndef weighted_cov(S, w):\n    \"\"\"Compute weighted covariance matrix.\"\"\"\n    mu = weighted_mean(S, w)\n    Xc = S - mu\n    return (Xc * w[:, None]).T @ Xc\n\n\ndef compute_metrics(X_full, S, w, method_name):\n    \"\"\"Compute all metrics comparing coreset to full data.\"\"\"\n    # Joint distribution metrics\n    mu_full = X_full.mean(axis=0)\n    cov_full = np.cov(X_full, rowvar=False)\n    \n    mu_coreset = weighted_mean(S, w)\n    cov_coreset = weighted_cov(S, w)\n    \n    mean_err = np.linalg.norm(mu_full - mu_coreset)\n    cov_err = np.linalg.norm(cov_full - cov_coreset, ord='fro')\n    \n    # Correlation matrices\n    std_full = np.sqrt(np.diag(cov_full))\n    std_core = np.sqrt(np.diag(cov_coreset))\n    corr_full = cov_full / (std_full[:, None] * std_full[None, :] + 1e-12)\n    corr_core = cov_coreset / (std_core[:, None] * std_core[None, :] + 1e-12)\n    corr_err = np.linalg.norm(corr_full - corr_core, ord='fro')\n    \n    # Marginal distribution metrics\n    d = X_full.shape[1]\n    W1_dims = []\n    KS_dims = []\n    \n    for dim in range(d):\n        W1 = wasserstein_1d_approx(X_full[:, dim], S[:, dim], w)\n        KS = ks_1d_approx(X_full[:, dim], S[:, dim], w)\n        W1_dims.append(W1)\n        KS_dims.append(KS)\n    \n    return {\n        'method': method_name,\n        'mean_err_l2': mean_err,\n        'cov_err_fro': cov_err,\n        'corr_err_fro': corr_err,\n        'W1_mean': np.mean(W1_dims),\n        'W1_max': np.max(W1_dims),\n        'KS_mean': np.mean(KS_dims),\n        'KS_max': np.max(KS_dims),\n    }\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for all methods\n",
    "metrics_ddc = compute_metrics(X, S_ddc, w_ddc, 'DDC')\n",
    "metrics_random = compute_metrics(X, S_random, w_random, 'Random')\n",
    "metrics_strat = compute_metrics(X, S_strat, w_strat, 'Stratified')\n",
    "\n",
    "# Create comparison table\n",
    "results_df = pd.DataFrame([metrics_ddc, metrics_random, metrics_strat])\n",
    "results_df = results_df.set_index('method')\n",
    "\n",
    "print(\"\\nDistributional Metrics Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Compute relative improvement\n",
    "print(\"\\nDDC Improvement over Random:\")\n",
    "print(\"=\" * 60)\n",
    "for metric in ['mean_err_l2', 'cov_err_fro', 'corr_err_fro', 'W1_mean', 'KS_mean']:\n",
    "    random_val = metrics_random[metric]\n",
    "    ddc_val = metrics_ddc[metric]\n",
    "    improvement = (1 - ddc_val / random_val) * 100\n",
    "    print(f\"{metric:20s}: {improvement:6.1f}% better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "Let's visualize the coresets and compare their distributional fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project coresets to 2D using same reducer",
    "S_ddc_2d = reducer.transform(S_ddc)",
    "S_random_2d = reducer.transform(S_random)",
    "S_strat_2d = reducer.transform(S_strat)",
    "",
    "# Get cluster labels for coreset points",
    "from sklearn.neighbors import NearestNeighbors",
    "nn = NearestNeighbors(n_neighbors=1)",
    "nn.fit(X)",
    "",
    "_, idx_ddc = nn.kneighbors(S_ddc)",
    "_, idx_random = nn.kneighbors(S_random)",
    "_, idx_strat = nn.kneighbors(S_strat)",
    "",
    "labels_ddc = cluster_labels[idx_ddc.flatten()]",
    "labels_random = cluster_labels[idx_random.flatten()]",
    "labels_strat = cluster_labels[idx_strat.flatten()]",
    "",
    "# Plot spatial coverage",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))",
    "",
    "for ax, S_2d, labels, title, w in zip(",
    "    axes, [S_ddc_2d, S_random_2d, S_strat_2d],",
    "    [labels_ddc, labels_random, labels_strat],",
    "    ['DDC', 'Random', 'Stratified'],",
    "    [w_ddc, w_random, w_strat]",
    "):",
    "    scatter = ax.scatter(S_2d[:, 0], S_2d[:, 1], c=labels, ",
    "                         cmap='viridis', s=w*1000, alpha=0.7, edgecolors='black', linewidths=0.5)",
    "    ax.set_title(f'{title} Coreset (k={len(S_2d)})', fontsize=12, fontweight='bold')",
    "    ax.set_xlabel('Component 1')",
    "    ax.set_ylabel('Component 2')",
    "    ax.grid(True, alpha=0.3)",
    "",
    "plt.tight_layout()",
    "",
    "# Save figure",
    "import os",
    "os.makedirs('images/tutorials/basic_tabular', exist_ok=True)",
    "plt.savefig('images/tutorials/basic_tabular/spatial_coverage_comparison.png', dpi=150, bbox_inches='tight')",
    "",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot marginal distributions for first 4 features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for dim in range(4):\n",
    "    ax = axes[dim]\n",
    "    \n",
    "    # Full data histogram\n",
    "    ax.hist(X[:, dim], bins=50, alpha=0.3, color='gray', label='Full Data', density=True)\n",
    "    \n",
    "    # Weighted histograms for coresets\n",
    "    for S, w, label, color in [\n",
    "        (S_ddc, w_ddc, 'DDC', 'blue'),\n",
    "        (S_random, w_random, 'Random', 'orange'),\n",
    "        (S_strat, w_strat, 'Stratified', 'green')\n",
    "    ]:\n",
    "        # Sample from weighted distribution\n",
    "        n_samples = 5000\n",
    "        probs = w / w.sum()\n",
    "        idx = np.random.choice(len(S), size=n_samples, p=probs, replace=True)\n",
    "        ax.hist(S[idx, dim], bins=30, alpha=0.5, label=label, \n",
    "                color=color, density=True, histtype='step', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f'Feature {dim+1} Marginal Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure\n",
    "import os\n",
    "os.makedirs('images/tutorials/basic_tabular', exist_ok=True)\n",
    "plt.savefig('images/tutorials/basic_tabular/spatial_coverage_comparison.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2D projection comparing spatial coverage of DDC, Random, and Stratified coresets](images/tutorials/basic_tabular/spatial_coverage_comparison.png)\n",
    "\n",
    "*Spatial coverage comparison: DDC (left), Random (center), and Stratified (right) coresets. Point sizes are proportional to weights. DDC ensures all clusters are represented, even small ones.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metrics_to_plot = ['mean_err_l2', 'cov_err_fro', 'W1_mean', 'KS_mean']\n",
    "methods = ['DDC', 'Random', 'Stratified']\n",
    "colors = ['blue', 'orange', 'green']\n",
    "\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    metrics = [results_df.loc[method, m] for m in metrics_to_plot]\n",
    "    axes[0].bar(x + i*width, metrics, width, label=method, color=colors[i], alpha=0.7)\n",
    "\n",
    "axes[0].set_xlabel('Metric')\n",
    "axes[0].set_ylabel('Error (lower is better)')\n",
    "axes[0].set_title('Distributional Metrics Comparison', fontweight='bold')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(metrics_to_plot, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Improvement percentages\n",
    "improvements = []\n",
    "for metric in metrics_to_plot:\n",
    "    random_val = results_df.loc['Random', metric]\n",
    "    ddc_val = results_df.loc['DDC', metric]\n",
    "    improvement = (1 - ddc_val / random_val) * 100\n",
    "    improvements.append(improvement)\n",
    "\n",
    "axes[1].bar(range(len(metrics_to_plot)), improvements, color='blue', alpha=0.7)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Metric')\n",
    "axes[1].set_ylabel('Improvement (%)')\n",
    "axes[1].set_title('DDC Improvement over Random', fontweight='bold')\n",
    "axes[1].set_xticks(range(len(metrics_to_plot)))\n",
    "axes[1].set_xticklabels(metrics_to_plot, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save figure\n",
    "import os\n",
    "os.makedirs('images/tutorials/basic_tabular', exist_ok=True)\n",
    "plt.savefig('images/tutorials/basic_tabular/metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### When DDC is Better\n",
    "\n",
    "- **Clustered data**: DDC preserves cluster structure better than Random\n",
    "- **Spatial coverage**: DDC ensures all clusters are represented\n",
    "- **Distributional fidelity**: DDC better preserves marginal distributions\n",
    "\n",
    "### When Random Might Be Better\n",
    "\n",
    "- **Very large datasets** (n >> k) with complex non-Gaussian structure\n",
    "- **Preserving exact global covariance** is critical\n",
    "- **High-dimensional sparse data**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try `multimodal_clusters.ipynb` for more complex cluster structures\n",
    "- Try `adaptive_distances.ipynb` for advanced features (presets, adaptive distances)\n",
    "- See `docs/DDC_ADVANTAGE_CASES.md` for comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram comparison of marginal distributions for DDC, Random, and Stratified coresets](images/tutorials/basic_tabular/marginal_distributions.png)\n",
    "\n",
    "*Marginal distribution comparison for the first 4 features. Gray histogram shows full data; colored lines show weighted coreset distributions. DDC better preserves the shape of marginal distributions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bar charts comparing distributional metrics across methods](images/tutorials/basic_tabular/metrics_comparison.png)\n",
    "\n",
    "*Distributional metrics comparison (left) and DDC improvement over Random (right). DDC excels at mean preservation but may trade off some covariance accuracy for better cluster coverage.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}