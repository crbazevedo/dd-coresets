{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# High-Dimensional Data: Automatic PCA Reduction\n",
        "\n",
        "This notebook demonstrates how DDC handles **high-dimensional data** (d \u2265 30) ",
        "using automatic PCA reduction. We'll show the `auto` mode in action.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How DDC handles high-dimensional data\n",
        "- Automatic dimensionality reduction (PCA)\n",
        "- Performance trade-offs\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "We'll use a **high-dimensional dataset** (d=60, n=20k) where auto mode triggers PCA reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dd-coresets (uncomment if needed)\n",
        "# !pip install dd-coresets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "try:\n",
        "    import umap\n",
        "    HAS_UMAP = True\n",
        "except ImportError:\n",
        "    HAS_UMAP = False\n",
        "    print(\"UMAP not available, using PCA for visualization\")\n",
        "\n",
        "from dd_coresets import fit_ddc_coreset, fit_random_coreset\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generate High-Dimensional Dataset\n",
        "\n",
        "We'll create a dataset with **60 features** and **20,000 samples**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate high-dimensional data\n",
        "n_samples = 20000\n",
        "n_features = 60  # High dimensionality\n",
        "n_clusters = 5\n",
        "\n",
        "X, cluster_labels = make_blobs(\n",
        "    n_samples=n_samples,\n",
        "    n_features=n_features,\n",
        "    centers=n_clusters,\n",
        "    cluster_std=1.5,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of features: {n_features} (high-dimensional)\")\n",
        "print(f\"Number of clusters: {n_clusters}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compare Modes: Euclidean vs Auto (with PCA)\n",
        "\n",
        "We'll compare Euclidean mode (no PCA) with Auto mode (triggers PCA for d \u2265 30)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "k = 500\n",
        "\n",
        "print(\"Fitting coresets...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Euclidean (no PCA, works but slower)\n",
        "start = time.time()\n",
        "S_euclidean, w_euclidean, info_euclidean = fit_ddc_coreset(\n",
        "    X, k=k, mode='euclidean', preset='balanced', random_state=RANDOM_STATE\n",
        ")\n",
        "time_euclidean = time.time() - start\n",
        "print(f\"\u2713 Euclidean: {len(S_euclidean)} representatives\")\n",
        "print(f\"  Time: {time_euclidean:.2f}s\")\n",
        "print(f\"  Pipeline: {info_euclidean['pipeline']}\")\n",
        "\n",
        "# Auto (should trigger PCA)\n",
        "start = time.time()\n",
        "S_auto, w_auto, info_auto = fit_ddc_coreset(\n",
        "    X, k=k, mode='auto', preset='balanced', random_state=RANDOM_STATE + 1\n",
        ")\n",
        "time_auto = time.time() - start\n",
        "print(f\"\u2713 Auto: {len(S_auto)} representatives\")\n",
        "print(f\"  Time: {time_auto:.2f}s\")\n",
        "print(f\"  Pipeline: {info_auto['pipeline']}\")\n",
        "if info_auto['pipeline']['pca_used']:\n",
        "    print(f\"  PCA: {info_auto['pipeline']['d_effective']} components \"\n",
        "          f\"(explained variance: {info_auto.get('pca_info', {}).get('explained_variance_ratio', 'N/A')})\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Metrics\n",
        "\n",
        "We'll compare distributional preservation and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def weighted_mean(S, w):\n",
        "    return (S * w[:, None]).sum(axis=0)\n",
        "\n",
        "def weighted_cov(S, w):\n",
        "    mu = weighted_mean(S, w)\n",
        "    Xc = S - mu\n",
        "    return (Xc * w[:, None]).T @ Xc\n",
        "\n",
        "def compute_metrics(X_full, S, w, method_name):\n",
        "    mu_full = X_full.mean(axis=0)\n",
        "    cov_full = np.cov(X_full, rowvar=False)\n",
        "    mu_coreset = weighted_mean(S, w)\n",
        "    cov_coreset = weighted_cov(S, w)\n",
        "    mean_err = np.linalg.norm(mu_full - mu_coreset)\n",
        "    cov_err = np.linalg.norm(cov_full - cov_coreset, ord='fro')\n",
        "    return {\n",
        "        'method': method_name,\n",
        "        'mean_err_l2': mean_err,\n",
        "        'cov_err_fro': cov_err,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics\n",
        "metrics_euclidean = compute_metrics(X, S_euclidean, w_euclidean, 'Euclidean')\n",
        "metrics_auto = compute_metrics(X, S_auto, w_auto, 'Auto (PCA)')\n",
        "\n",
        "results_df = pd.DataFrame([metrics_euclidean, metrics_auto])\n",
        "results_df = results_df.set_index('method')\n",
        "\n",
        "print(\"Distributional Metrics:\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.round(4))\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Euclidean: {time_euclidean:.2f}s\")\n",
        "print(f\"  Auto (PCA): {time_auto:.2f}s\")\n",
        "speedup = time_euclidean / time_auto if time_auto > 0 else 0\n",
        "print(f\"  Speedup: {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "Let's visualize the coresets in 2D and PCA explained variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project to 2D\n",
        "if HAS_UMAP:\n",
        "    reducer = umap.UMAP(n_components=2, random_state=RANDOM_STATE)\n",
        "else:\n",
        "    reducer = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "\n",
        "X_2d = reducer.fit_transform(X)\n",
        "S_euclidean_2d = reducer.transform(S_euclidean)\n",
        "S_auto_2d = reducer.transform(S_auto)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for ax, S_2d, title in zip(\n",
        "    axes, [S_euclidean_2d, S_auto_2d],\n",
        "    ['Euclidean (No PCA)', 'Auto (With PCA)']\n",
        "):\n",
        "    ax.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, \n",
        "              cmap='viridis', alpha=0.1, s=5)\n",
        "    ax.scatter(S_2d[:, 0], S_2d[:, 1], c='red', s=30, alpha=0.8, \n",
        "              edgecolors='black', linewidths=0.5)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Component 1')\n",
        "    ax.set_ylabel('Component 2')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show PCA explained variance if available\n",
        "if info_auto['pipeline']['pca_used']:\n",
        "    pca_info = info_auto.get('pca_info', {})\n",
        "    if pca_info and 'explained_variance_ratio' in pca_info:\n",
        "        evr = pca_info['explained_variance_ratio']\n",
        "        if isinstance(evr, (list, np.ndarray)):\n",
        "            cumvar = np.cumsum(evr)\n",
        "            \n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(range(1, len(cumvar) + 1), cumvar, 'o-', linewidth=2, markersize=6)\n",
        "            plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
        "            plt.xlabel('Number of Components')\n",
        "            plt.ylabel('Cumulative Explained Variance')\n",
        "            plt.title('PCA Explained Variance (Auto Mode)', fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            print(f\"\\nPCA Summary:\")\n",
        "            print(f\"  Original dimensions: {n_features}\")\n",
        "            print(f\"  Reduced dimensions: {len(evr)}\")\n",
        "            print(f\"  Explained variance: {cumvar[-1]:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Key Takeaways\n",
        "\n",
        "### When Auto Mode Triggers PCA\n",
        "\n",
        "- **d \u2265 30**: Auto mode automatically applies PCA reduction\n",
        "- **Retains 95% variance** by default (configurable)\n",
        "- **Faster computation** in reduced space\n",
        "- **Representatives always in original space**\n",
        "\n",
        "### Performance Benefits\n",
        "\n",
        "- PCA reduction speeds up computation\n",
        "- Better density estimation in lower dimensions\n",
        "- Maintains distributional fidelity\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try `adaptive_distances.ipynb` for adaptive distance features\n",
        "- Try `label_aware_classification.ipynb` for supervised problems\n",
        "- See `docs/ADAPTIVE_DISTANCES_EXPLAINED.md` for technical details"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}